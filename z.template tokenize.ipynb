{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3de194",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c619379c",
   "metadata": {},
   "source": [
    "# HOW TO TOKENIZE A TEXT AND COUNT UNIGRAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aea9b9",
   "metadata": {},
   "source": [
    "## Preparing a text for tokenization\n",
    "\n",
    "You must have a .txt file which has already been pre-processed. You put the file in a folder in your Python environment and you open the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a statement to give a variable name to the file. The file must be indicate as 'folder_name/file_name'\n",
    "text_name = 'data/apocalisse.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c03bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening a file in read mode. \n",
    "## If the text is too long, it could be better to not print it because there might be crash errors\n",
    "### If you are using Constellate, the right environment is all inside the folder 'constellate-notebooks'.\n",
    "with open(text_name, 'r') as f:\n",
    "    text = f.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ff33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the raw string version of our text (i.e. in a sigle line, without tabs, paragraphs etc...)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e8e78",
   "metadata": {},
   "source": [
    "## Splitting a text for tokenization\n",
    "\n",
    "Now you split the text in unigrams and you create a clean and correct list of unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the text string into a list of strings. \n",
    "# To choose multiple delimiters (empty spaces and apostrophes), we  use re.split function, but this function don't recognize the /n espace character (so we must add it as delimiters) \n",
    "## If you choose a single delimiter (for ex. empty spaces or commas) you can use the most simple .split method:\n",
    "##tokenized_list = text.split()\n",
    "##list(tokenized_list)\n",
    "\n",
    "import re\n",
    "tokenized_list = re.split(\" |'|\\n\", text)\n",
    "list(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the empty items from the list.\n",
    "#The empty items has been generated using \\n as delimiter in the re.split function\n",
    "#This is the system to remove not only the first but all the empty spaces.\n",
    "while '' in tokenized_list:\n",
    "    tokenized_list.remove('')\n",
    "    \n",
    "list(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d65e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the tokens\n",
    "unigrams = []\n",
    "\n",
    "for token in tokenized_list:\n",
    "    token = token.lower() # lowercase tokens ## if some cases, the capital letters are a distinctive marks (ex. brown is a color and Brown is a surname), you have to differently rename one of the two cases (ex. brown and name-brown). \n",
    "    token = token.replace('.', '') # remove periods\n",
    "    token = token.replace('!', '') # remove exclamation points\n",
    "    token = token.replace('?', '') # remove question marks\n",
    "    token = token.replace(',', '')\n",
    "    token = token.replace(':', '')\n",
    "    token = token.replace(';', '')\n",
    "    token = token.replace('(', '')\n",
    "    token = token.replace(')', '')\n",
    "    \n",
    "    unigrams.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the unigrams\n",
    "list(unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d6d30",
   "metadata": {},
   "source": [
    "## Counting the occurrences of unigrams with a word counter\n",
    "\n",
    "Now you count the occurences of the unigrams. In this way you collect the tokens in types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3733a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import, from a Python Library, the Counter function and count up the tokens using a Counter() object\n",
    "from collections import Counter\n",
    "word_counts = Counter(unigrams)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking to the most common unigrams.\n",
    "word_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the most common unigrams visualization\n",
    "for gram, count in word_counts.most_common(50):\n",
    "    print(gram.ljust(20), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126351a",
   "metadata": {},
   "source": [
    "## Loading a stop word list and clean the unigrams\n",
    "\n",
    "Now you clean the unigram count from function words (articles, prepositions, conjuctions ecc...), from non-alphabethic unigrams and you re count the cleaned unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b049a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff53c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a custom stop_words.csv, if available, in a folder of your Python environment\n",
    "# Otherwise, load the nltk stopwords list in your language (english, italian, ecc..)\n",
    "\n",
    "# Create an empty Python list to hold the stopwords\n",
    "stop_words = []\n",
    "\n",
    "# The filename of the custom data/stop_words.csv file\n",
    "stopwords_list_filename = 'data/stop_words_italian.csv'\n",
    "\n",
    "if os.path.exists(stopwords_list_filename):\n",
    "    import csv\n",
    "    with open(stopwords_list_filename, 'r') as f:\n",
    "        stop_words = list(csv.reader(f))[0]\n",
    "    print('Custom stopwords list loaded from CSV')\n",
    "else:\n",
    "    # Load the NLTK stopwords list. You can choose a different language\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('italian')\n",
    "    print('NLTK stopwords list loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview stop words\n",
    "list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gram, count in word_counts.items():\n",
    "    clean_gram = gram.lower()\n",
    "    if clean_gram in stop_words:\n",
    "        continue\n",
    "    print(clean_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing from our previous unigram counts (word_counts): the loaded stop_words, the token that aren't composed only by alphabetic letters and the too short token. \n",
    "\n",
    "##assigning the name 'word_frequency' to the Counter function. This statement will be useful for creating a new list\n",
    "word_frequency = Counter()\n",
    "\n",
    "for gram, count in word_counts.items():\n",
    "        clean_gram = gram.lower()  # from our previous unigrams count, you consider only unigrams in lower case (without count)\n",
    "        if clean_gram in stop_words:\n",
    "            continue               # if this condition is True (unigram = stop word), the unigram remain in the loop and it doesn't pass over             \n",
    "        if not clean_gram.isalpha():\n",
    "            continue                # if this condition is True (unigram with not only alphabetic letters), the unigram remain in the loop and it doesn't pass over\n",
    "        if len(clean_gram) < 2:\n",
    "            continue              # if this condition is True (unigram <2), the unigram remain in the loop and it doesn't pass over\n",
    "        word_frequency[clean_gram] += count #every unigram that is pass over is considered and added up to the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0edc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c3da3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for gram, count in word_frequency.most_common(100):\n",
    "    print(gram.ljust(20), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb8582",
   "metadata": {},
   "source": [
    "## Asking for a single unigram count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556509ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with the word 'vino' \n",
    "word_frequency.get('vino','not found word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2574d",
   "metadata": {},
   "source": [
    "## Creating a word cloud\n",
    "\n",
    "We create a word cloud based on the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4554139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download cloud image for our word cloud shape\n",
    "import urllib.request\n",
    "download_url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/sample_cloud.png'\n",
    "urllib.request.urlretrieve(download_url, './data/sample_cloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud from our data\n",
    "\n",
    "# Adding a mask shape of a cloud to your word cloud\n",
    "# By default, the shape will be a rectangle\n",
    "# You can specify any shape you like based on an image file\n",
    "cloud_mask = np.array(Image.open('./data/sample_cloud.png')) # Specifies the location of the mask shape\n",
    "cloud_mask = np.where(cloud_mask > 3, 255, cloud_mask) # this line will take all values greater than 3 and make them 255 (white)\n",
    "\n",
    "### Specify word cloud details\n",
    "wordcloud = WordCloud(\n",
    "    width = 800, # Change the pixel width of the image if blurry\n",
    "    height = 600, # Change the pixel height of the image if blurry\n",
    "    background_color = \"white\", # Change the background color\n",
    "    colormap = 'viridis', # The colors of the words, see https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    max_words = 150, # Change the max number of words shown\n",
    "    min_font_size = 4, # Do not show small text\n",
    "    \n",
    "    # Add a shape and outline (known as a mask) to your wordcloud\n",
    "    contour_color = 'blue', # The outline color of your mask shape\n",
    "    mask = cloud_mask, # \n",
    "    contour_width = 1\n",
    ").generate_from_frequencies(word_frequency)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (20,20) # Change the image size displayed\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
