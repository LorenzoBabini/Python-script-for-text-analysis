{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b0b860b",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://upload.wikimedia.org/wikipedia/commons/e/e1/CC_BY_icon.svg\"><br />\n",
    "\n",
    "Created by Lorenzo Babini and under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email lorenzo.babini@unicatt.it.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c619379c",
   "metadata": {},
   "source": [
    "# HOW TO TOKENIZE A TEXT AND COUNT BIGRAMS AND TRIGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a statement to give a variable name to the file. The file must be indicate as 'folder_name/file_name'\n",
    "text_name = 'data/apocalisse.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c03bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening a file in read mode. \n",
    "## If the text is too long, it could be better to not print it because there might be crash errors\n",
    "### If you are using Constellate, the right environment is all inside the folder 'constellate-notebooks'.\n",
    "with open(text_name, 'r') as f:\n",
    "    text = f.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the text string into a list of strings. \n",
    "# To choose multiple delimiters (empty spaces and apostrophes), we  use re.split function, but this function don't recognize the /n espace character (so we must add it as delimiters) \n",
    "## If you choose a single delimiter (for ex. empty spaces or commas) you can use the most simple .split method:\n",
    "##tokenized_list = text.split()\n",
    "##list(tokenized_list)\n",
    "\n",
    "import re\n",
    "tokenized_list = re.split(\" |'|\\n\", text)\n",
    "list(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the empty items from the list.\n",
    "#The empty items has been generated using \\n as delimiter in the re.split function\n",
    "#This is the system to remove not only the first but all the empty spaces.\n",
    "while '' in tokenized_list:\n",
    "    tokenized_list.remove('')\n",
    "    \n",
    "list(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d65e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the tokens\n",
    "unigrams_list = []\n",
    "\n",
    "for token in tokenized_list:\n",
    "    token = token.lower() # lowercase tokens ## if some cases, the capital letters are a distinctive marks (ex. brown is a color and Brown is a surname), you have to differently rename one of the two cases (ex. brown and name-brown). \n",
    "    token = token.replace('(', '')\n",
    "    token = token.replace(')', '')\n",
    "    token = token.replace('.', '')\n",
    "    token = token.replace(';', '')\n",
    "    token = token.replace(',', '')\n",
    "    token = token.replace(':', '')\n",
    "    token = token.replace('!', '')\n",
    "    token = token.replace('?', '')\n",
    "    token = token.replace('\"', '')\n",
    "    \n",
    "    unigrams_list.append(token)\n",
    "    \n",
    "print(unigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing a function for automatic tokenization from the Natural Language Tool Kit Library\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805635b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our bigrams and trigrams\n",
    "bigrams = list(nltk.bigrams(unigrams_list))\n",
    "trigrams = list(nltk.trigrams(unigrams_list))\n",
    "\n",
    "print('Bigrams: \\n ', bigrams, '\\n')\n",
    "    \n",
    "print('Trigrams: \\n,', trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions for Converting NLTK tuples into strings\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def convert_tuple_bigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into bigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        gram_string = f'{first_word} {second_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_tuple_trigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into trigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        third_word = tuple_grams[2]\n",
    "        gram_string = f'{first_word} {second_word} {third_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_strings_to_counts(string_grams):\n",
    "    \"\"\"Converts a Counter of n-grams into a dictionary\"\"\"\n",
    "    counter_of_grams = Counter(string_grams)\n",
    "    dict_of_grams = dict(counter_of_grams)\n",
    "    return dict_of_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9338817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the tuples\n",
    "string_bigrams = convert_tuple_bigrams(bigrams)\n",
    "bigramCount = convert_strings_to_counts(string_bigrams)\n",
    "\n",
    "print('Bigrams as a dictionary of counts')\n",
    "print(bigramCount, '\\n')\n",
    "\n",
    "string_trigrams = convert_tuple_trigrams(trigrams)\n",
    "trigramCount = convert_strings_to_counts(string_trigrams)\n",
    "\n",
    "print('Trigrams as a dictionary of counts')\n",
    "print(trigramCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting bigrams, to look at the most common ones.\n",
    "sort_bigramCount = sorted(bigramCount.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_bigramCount:\n",
    "    print(i[0].ljust(20), i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8871fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting trigrams, to look at the most common ones.\n",
    "sort_trigramCount = sorted(trigramCount.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i in sort_trigramCount:\n",
    "    print(i[0].ljust(27), i[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
